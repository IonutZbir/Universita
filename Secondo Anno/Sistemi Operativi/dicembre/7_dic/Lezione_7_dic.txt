-> Performance del File System

C'è un gap di velocità di accesso troppo grande tra RAM e memoria volatile, negli ultimi anni questo gap è stato minimizzato con l'invenzione degli SSD. Per leggere una parola a 32 bit, la RAM ci mette circa 10ns mentre un disco magnetico ci mette circa 10ms. Pertanto, bisogna progettare un file system con diverse ottimizzazioni per migliorare le prestazioni, considerando le significative differenze nel tempo di accesso e riducendo al minimo i numeri di accesso al disco/SSD.

Ci sono due tecniche principali per ottimizzare il file system. 

# Uso della Cache
Utilizzata per ridurre i tempi di accesso al disco mantenendo i blocchi più usati in memoria. La cache si trova all'interno della memoria RAM.

Concetti di Caching 
- Buffer Cache: Memorizza i blocchi del disco in RAM per ridurre gli accessi al disco
- Page Cache: Memorizza le pagine del filesystem virtuale(VFS) in RAM prima di passare al driver del dispositivo. Sono struttura di intermezzo tra il virtual file system e il vero file system.
Le cache devono essere ottimizzate in modo tale che non ci siano file duplicitati, perche puo succedere che buffer cache e page cache spesso contengono gli stessi dati, per esempio file "mappati in memoria".

In poche parole i file sono nella cache delle pagine e i blocchi del disco sono nella cache buffer.
I sistemi operativi possono combinare buffer cache e page cache per un uso più efficiente della memoria e una riduzione degli accessi al disco.

Implmenetazione della cache.
Per la gestione della cache si puo usare il seguente algoritmo: si controllano tutte le richieste di lettura per verificare se il blocco necessario sia nella cache. Se lo è, la richiesta di lettura è soddisfatta senza accedere al disco. Se non lo è, per prima cosa viene letto da disco e portato nella cache, quindi copiato ovunque ve ne sia bisogno. Le successive richieste per lo stesso blocco potranno essere soddisfatte dalla cache.
Generalmente nella cache vi sono molti blocchi e serve un metodo rapido per determinare se un blocco sia o meno nella cache. Viene quindi utilizzata una hash table per velocizzare la ricerca di un blocco all'interno di un blocco.

{immagine}

Quando un blocco deve essere caricato in una cache piena, ne deve essere rimosso qualcuno. Questa situazione è molto simile alla paginazione e sono applicabili tutti gli abituali algoritmi di sostituzione delle pagine, come FIFO, seconda chance ed LRU. Una gradevole differenza fra l’uso della paginazione e della cache sta nel fatto che i riferimenti alla cache sono relativamente poco frequenti, così è possibile mantenere tutti i blocchi nell’esatto ordine LRU con le liste concatenate.
Nella Figura si osserva che oltre alle catene che partono dalla tabella di hash c’è anche una lista bidirezionale che collega tutti i blocchi in ordine d’uso, con il blocco usato meno recentemente in testa alla lista e il blocco usato più recentemente in fondo alla lista. In questo modo può essere mantenuto l’esatto ordine LRU.

C'è un unico problema pero: LRU puo portare a incosistenza in caso di crash, specialmente per blocchi critici come gli i-node. Per esempio blocco di i-node, viene letto nella cache e modificato, ma non riscritto su disco, un crash del file system lo lascerebbe in uno stato incoerente. Se il blocco di i-node viene posto alla fine della catena LRU, può volerci parecchio tempo prima che raggiunga la testa e sia riscritto sul disco.
Per risolvere a questo problema si puo pensare a modificare LRU, dividendo i blocchi per categorie basate sulla loro importanza.
Se un blocco è fondamentale per la coerenza del file system, questo blocco dovrebbe essere scritto immediatamente sul disco, indipendentemente dalla sua posizione nella LRU.

Oltre a queste tecniche, UNIX mette a disposizione una chiamata di sistema 'sync' che forza la scrittura di tutti blocchi modificati. Al momento dell'avvio del sistema, viene eseguito un processo in background che ad ogni 30 secondi scrive tutti i blocchi modificati su disco. Ad oggi esiste una chiamata simile anche in Windows, ma prima veniva usata un'altra tecnica detta write-through, ovvero il sistema scriveva su disco ogni blocco che veniva modificato appena veniva scritto in cache.

Una conseguenza di tale differenza nella strategia di caching è che la rimozione di un disco da un sistema UNIX senza fare la sync (per questo è opportuno prima di togliere un disco, fare umount) provoca quasi certamente una perdita di dati, e spesso intacca anche il file system. Con le cache write-through il problema non si pone. 

Concettualmente cache page e cache buffer sono diversi nel senso che la cache delle pagine contiene le pagine dei file per ottimizzare l’I/O, mentre la cache buffer contiene semplicemente blocchi del disco. La cache buffer, nata prima della cache delle pagine, in realtà si comporta un po’ come un disco, tranne che letture e scritture avvengono nella memoria. Il motivo per cui è stata aggiunta una cache delle pagine è che sembrava una buona idea portare la cache più in alto nello stack, in modo che le richieste di file potessero essere soddisfatte senza passare per il codice del file system e tutte le sue complessità. In altre parole: i file sono nella cache delle pagine e i blocchi del disco nella cache buffer.

Alcuni sistemi operativi combinano cache buffer con cache page per una gestione efficiente dei dati. Inoltre supportano i file mappati in memoria, trattando allo stesso modo pagine e blocchi del disco, in un'unica cache.

# Allocazione dei Blocchi e Read Ahead
Un’altra tecnica importante per i dischi magnetici è quella di ridurre la quantità di movimenti del braccio mettendo vicini, preferibilmente nello stesso cilindro, i blocchi ai quali è probabile si acceda in sequenza. Quando viene scritto un file di output, il file system deve allocare i blocchi uno alla volta, a richiesta. Se i blocchi liberi sono registrati in una bitmap e l’intera bitmap è nella memoria principale, è abbastanza semplice scegliere un blocco libero che sia il più vicino possibile al blocco precedente. Se invece si usa la lista dei blocchi liberi, parte della quale sta su disco, è molto più difficile allocare blocchi vicini l’uno all’altro.
Altro collo di bottiglia delle prestazioni nei sistemi che usano gli i-node, o qualcosa di analogo, è che la lettura di un file, anche piccolo, richiede due accessi al disco: uno per l’i-node e uno per il blocco.
{immagine}
a) Posizionamento tradizionale degli I-node: I-node vicino all'inizio del disco, risultando in ricerche più lunghe.
b) Centrare gli I-node: Posizionare gli I-node nel mezzo del disco per dimezzare il tempo di ricerca. Dividere il disco ingruppi di cilindri con I-node, blocchi e lista dei blocchi liberi per ciascun gruppo.

Ovviamente, i movimenti del braccio del disco e il tempo di rotazione sono importanti solamente se il disco è magnetico; non sono rilevanti per gli SSD, che non hanno parti mobili. Per questi dischi, costruiti con la tecnologia delle schede flash, l’accesso casuale (in lettura) è rapido quanto quello sequenziale

# Deframmentazione dei dischi
 con il passare del tempo, i file vengono creati ed eliminati e di norma il disco diventa molto frammentato, con file e buchi sparsi ovunque. Di conseguenza, quando viene creato un nuovo file i blocchi utilizzati potrebbero essere sparpagliati per tutto il disco, ottenendo basse prestazioni.

Le prestazioni possono essere ripristinate spostando i file in modo che siano contigui e mettendo tutto (o la maggior parte) dello spazio libero in una o più grandi zone continuesu disco. Windows ha un programma, defrag, che fa esattamente questo. Gli utenti di Windows dovrebbero eseguirlo con regolarità, ma non sugli SSD.

I file system di linux (specialmente ext3, ext4, btrfs) gestiscono meglio la frammentazione, riducendo la necessità di deframmentazione manuale. Ext4 introduce la preallocazione dei blocchi. Quando si scrive un file, Ext4 prealloca un gruppo di blocchi contigui, piuttosto che uno alla volta, riducendo la frammentazione per i file in espansione. 

# Comprensione e Deduplicazione

Alcuni file system come NTFS, Btrfs possono comprimere dati automaticamente e scrivere sul disco i dati compressi per risparmiare spazio. Oltre a eliminare la ridondanza entro un singolo file, alcuni file system eliminano anche la ridondanza in tutti i file. Nei sistemi che memorizzano dati di molti utenti, ad esempio in un ambiente cloud o server, è facile trovare file che contengono gli stessi dati quando più utenti salvano gli stessi documenti, file binari o video. Anziché salvare più volte gli stessi dati, alcuni file system implementano la deduplicazione per eliminare i doppioni.
La deduplicazione può essere eseguita inline o post-process. Con la deduplicazione inline, il file system calcola uno hash per ogni chunk (“pezzo” di memoria) che sta per scrivere e lo confronta con gli hash dei chunk esistenti. Se il chunk è già presente, eviterà di scrivere i dati e aggiungerà invece un riferimento (hard link) al chunk esistente. Naturalmente i calcoli aggiuntivi richiedono tempo e rallentano la scrittura. La deduplicazione post-process, invece, scrive sempre i dati ed esegue l’hashing e i confronti in background, senza rallentare le operazioni di elaborazione dei file.

-> Affidabilità del File System

# Minacce alla affidabilità del File System
- Guasti del Disco, come blocchi danneggiati ovvero settori illeggibili che possono corrompere dati, oppure errori su intero disco, rendendo il disco inutilizzabile.
- Interruzioni di Energia, causando incongruenze nei dati o nei metadati sul disco.
- Bug del Software, errori di programmazione portano alla scrittura di dati errati/corrotti
- Errori Umani: rm *.o VS rm * .o; Nel primo caso vengono cancellati tutti i file che hanno estensione '.o', mentre nel secondo caso vengono cancellati tutti i file (*) e poi, se ci sono ancora tutti i file che finiscono con '.o'.
- Perdita o Furto del Computer, rischi di accesso ai dati da parte di persone non autorizzate in caso di furto o smarrimento del dispositivo.
- Malware/Ransomware, virus o altri software dannosi che possono infettare, criptare o distruggere dati.

# Backup
I backup su disco sono generalmente effettuati per affrontare uno dei due potenziali problemi: 
- RECUPERO DA UN DISASTRO 
    (crash del disco, incendio, catastrofe natulare)
- RECUPERO DALLA STUPIDITA'
    (eliminazione accidentale di file, per esempio in Windows esiste il cestino, una cartella in cui vanno i file 'eliminati', in modo poi da poterli recuperare)

Ci sono cinque punti fondamentali di cui tener conto.
1. Un backup impiega molto tempo e occupa molto spazio, per questo è importante farlo in modo efficiente e comodo. Di quali file bisogna fare il backup? Di tutto il file system o solo di alcune directory?

2. E' uno spreco rifare il backup di file che non sono cambiati dall’esecuzione dell’ultimo backup, e questo ci porta al concetto di backup incrementale. La forma incrementale più semplice consiste nel fare periodicamente un backup completo, diciamo una volta alla settimana o al mese, ed eseguire un backup giornaliero solo dei file modificati dopo l’ultimo backup completo. Il ripristino (restore) dei dati è più complicato, dato che va prima ripristinato il backup totale più recente, seguito poi dai backup incrementali in ordine inverso.

3. Poiché generalmente sono memorizzate grandi quantità di dati, potrebbe essere utile comprimere i dati prima di scriverli sul supporto di backup. Però con molti algoritmi di compressione basta un solo punto difettoso sul supporto per compromettere il risultato dell’algoritmo di compressione e rendere illeggibile l’intero file o l’intero supporto. Per questo la decisione di comprimere il flusso di backup va valutata attentamente.

4. E' difficile eseguire il backup di un file system attivo. Se durante il processo di backup vengono aggiunti, cancellati, modificati file e directory, il risultato potrebbe essere un backup non coerente. Per questo motivo sono stati definiti algoritmi che creano delle istantanee (snapshot) dello stato del file system.

5. I backup devono essere sempre tenuti al di fuori del luogo dove risiedono i computer, introducendo ulteriori rischi per la sicurezza (visto che adesso i luoghi da sorvegliare sono due). Questi aspetti organizzativi devono essere sempre presi in considerazione.

Le strategie utilizzte per effettuare backup su un disco di supporto sono due: backup fisico e backup logico.

- Backup Fisico
Copia sequenziale di tutti i blocchi del disco, a partire dal blocco 0 fino all'ultimo. E' semplice e veloce, puo essere eseguito alla velocità del disco. 
Necessità di evitare la copia di blocchi danneggiati per prevenire errori di lettura.
Necessità di evitare la copia di file di sistema come i file di paginazione e ibernazione (il file di ibernazione è il file in cui il SO va a fare un snapshot quando va in modalità suspend)
Limitazioni: Ha difficoltà nel saltare le directory specifiche o nel fare backup incrementali. Non è possibile inoltre ripristinare file individuali senza un intero ripristino del sistema.

- Backup Logico
Seleziona e copia solo i file e le directory specifici, ignorando i file di sistema e i blocchi danneggiati. 
Parte da una directory specifica e effettua il backup di tutti i file e directory modificati a partire da una data specifica. Permette il ripristino o il trasferimento dell'intero file system su un nuovo computer. Ideale per backup incrementali o completi.
Consente il ripristino semplice di file o directory specifici grazie alla precisa identificazione dei dati salvati.

Algoritmo di backup:
{immagine}
L'algoritmo consiste di 4 fasi:
- Fase 1. Rilevamento delle modifiche
Inizia dalla directory radice, esamina tutte le voci e contrassegna nella bitmap gli I-node di file e directory modificati. Include tutte le directory nel percorso verso file modificati, indipendentemente dal loro stato di modifica. 
{immagine}

- Fase 2. Pulizia della Bitmap 
Deseleziona le directory che non contengono ne file modificati ne sottodirectory modificate. Lascia nella bitmap solo gli elementi che richiedono il backup.
{immagine}

- Fase 3 e 4. Backup Effettivo
Fase 3: Backup delle directory contrassegnate, con i loro attributi
Fase 4: Backup dei file contrassegnati, sempre con i relativi attributi.

Per ripristinare un file system con un backup, si crea un nuovo file system vuoto, poi viene ripristinato il backup completo, seguito dai backup incrementali.
Alcune considerazioni sul backup logico.
1. Dato che la lista dei blocchi liberi non è un file, non è oggetto di backup e perciò deve essere ricostruita da zero alla fine del ripristino di tutti i backup. È sempre possibile farlo, poiché il set dei blocchi liberi è semplicemente complementare a quello dei blocchi occupati da tutti i file messi insieme.
2. Se un file è collegato a una o più directory tramite link, è importante che quel file sia ripristinato solo una volta e che tutte le directory che dovrebbero puntare a esso lo facciano.
3. Un’ulteriore questione è il fatto che i file UNIX possono contenere dei buchi. È consentito aprire un file, scriverci pochi byte, quindi spostarsi a un offset distante e scrivervi altri byte. I blocchi nel mezzo non sono parte del file, non dovrebbero essere salvati nel backup né ripristinati. Ogni file ripristinato riempirà quest’area con degli zero e avrà così la medesima dimensione dello spazio degli indirizzi virtuali.
4. File speciali chiamati "pipe" e altri file simili (in pratica, qualsiasi cosa non sia un “vero” file) non andrebbero mai salvati nel backup, indipendentemente dalla directory in cui si possano trovare.


