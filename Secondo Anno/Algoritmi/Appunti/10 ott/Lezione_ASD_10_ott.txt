*Modelli di calcolo

- Un modello storico: la macchina di Turing
{immagine}
	- troppo di basso livello
	- utile per calcoli, poco efficiente

- Modello RAM (Random Access Machine)
	- un programma finito
	- un nastro di input e uno di output
	- una memoria strutturata come array
	- una CPU per eseguire istruzioni
{immagine}
	
L'analisi della complessita di un algoritmo è basata sul concetto di passo elementare
Passi elementari su una RAM:
	- istruzione I/O
	- operazione aritmetico/logica
	- accesso/modifica della memoria

- Criteri di costo

# Criterio di costo uniforme (è usato generalmente):
	- tutte le operazioni hanno lo stesso costo
	- complessità temporale misurata come numero di passi elementari eseguiti
# Criterio di costo logaritmico:	
	- Il costo di una operazione dipende dagli operandi
	- Un operazione su un operando x ha costo log x
	- Modella meglio la complessità di algoritmi numerici

# Caso peggiore
	Si tempo(I) il tempo di esecuzione di un algoritmo sull'istanza I
	T_worst(n) = max_istanze_I_dim_n{tempo(I)}
	T_worst(n) è il tempo di esecuzione sulle istanze di ingresso che comportano più lavoro per l'algoritmo
	Rappresenta una garanzia sul tempo di esecuzione di ogni istanza

# Caso medio
	Sia P(I) la probabilità di occorenza dell'istanza I
	T_avg(n) = sum(_istanze_I_dim_n{P(I) * tempo(I)})
	T_avg(n) è il tempo di esecuzione nel caso medio, ovvero sulle istanze di ingresso tipiche per il problema
	Non è possibilie conoscore la distribuzione di probabilità sulle istanze, infatti si fanno delle assunzioni.

*Notazione Asintotica

T(n) = numero di passi elementari eseguiti su una RAM nel caso peggire su un'istanza di dimensione n
L'idea di base è di descrivere T(n) in modo qualitativo, ovvero perdere un po in precisione e guadagnare in semplicità, cioè ignoro: - costanti moltiplicative; - termini di ordine inferiore
Nota: l'assunzione implicità è che guardo come si comporta l'algoritmo su istanze grandi.

- O grande
DEF: f(n) = O(g(n)) se esistono due costanti c > 0 e n_0 > 0 tali che 0<=f(n)<=c*g(n) per ogni n >= n_0
{immagine}
Esempi:
Sia f(n) = 2n^2 + 3n, allora
# fn = O(n^3)
# fn = O(n^2)
# fn != O(n)

Nota: 
lim_n->inf fn/gn = 0 => fn = O(gn)
fn = O(gn) !=> lim_n->0 fn/gn = 0
fn = O(gn) => lim_n->inf fn/gn (se esiste) < inf 

- Omega Grande
DEF: f(n) = Omega(g(n)) se esistono due costanti c > 0 e n_0 > 0 tali che f(n)>=c*g(n)>=0 per ogni n >= n_0
{immagine}
Sia f(n) = 2n^2 - 3n, allora
# fn = Omega(n)
# fn = Omega(n^2)
# fn != Omega(n^3)

Nota: 
lim_n->inf fn/gn = inf => fn = Omega(gn)
fn = Omega(gn) !=> lim_n->0 fn/gn = inf
fn = O(gn) => lim_n->inf fn/gn (se esiste) > 0


- Theta
DEF: f(n) = theta(g(n)) se esistono tre costanti c1 > 0, c2 > 0 e n_0 > 0 tali che c1*g(n)<=f(n)<=c2*g(n) per ogni n >= n_0
{immagine}
Sia f(n) = 2n^2 - 3n, allora
# fn = theta(n^2)
# fn != theta(n)
# fn != theta(n^3)

Nota: 
fn = theta(gn) => fn = O(gn)
fn = O(gn) !=> fn = theta(gn)

fn = theta(gn) => fn = Omega(gn)
fn = Omega(gn) !=> fn = theta(gn)

fn = theta(gn) <=> fn = Omega(gn) e fn = O(gn)

- o Piccolo
DEF: fn = o(gn) <=> lim_n->inf fn/gn = 0
Nota:
o(gn) => O(gn)


- omega Piccolo
DEF: fn = omega(gn) <=> lim_n->inf fn/gn = inf
Nota:
omega(gn) => Omega(gn)

Alcune proprietà:

fn = O(gn) <=> gn = Omega(fn)
fn = o(gn) <=> gn = omega(fn)

Analogie:

O: <=
Omega: >=
Theta: =
o: <
omega: >

{imagine}

Alcuni limiti notevoli:

#Polinomi
P(n) = a_d*n^d + a_d-1*n^d-1 + .... + a_0 
=> Pn = theta(n^d)
=> Pn = O(n^d)
=> Pn = Omega(n^d)

#Esponenziali
fn = a^n
=> a^n = omega(n^d)
=> a^n = Omega(n^d)

#Logaritmi
fn = log_b(n)
=> log_b(n)^c = o(n^d)
=> log_b(n)^c = O(n^d)

#Fattoriali
fn = n!
=> n! = o(n^n)
=> n! = omega(a^n)

Velocità delle funzioni composte

# date fn e gn, la velocità ad andare a infinito della funzione fn + gn è la velocità della più veloce fra fn e gn
# date fn e gn, la velocità ad andare a infinito della funzuione fn * gn è la velocità di fn "più" la velocità di gn
# date fn e gn, la velocità ad andare a infinito della funzuione fn / gn è la velocità di fn meno" la velocità di gn

* Usare la notazione asintotica nelle analisi

- Analisi complessità fibonacci3
algoritmo fibonacci3(intero n) -> intero
	sia Fib un array di n interi
	Fib[1] = Fib[2] = 1
	for i = 3 to n do
		Fib[i] = Fib[i-1] + Fib[i-2]
	return Fib[n]

T(n): complessità computazionale nel caso peggiore con input n
c_j: numero di passi elementari eseguiti su una RAM quando è eseguita la linea di codice j

# Upper Bound
- linea 1, 3, 5 eseguite una sola volta
- linee 3 e 4 eseguite al più n volte

T(n) <= c1 + c3 + c5 + (c3 + c4)*n = Theta(n)
=> T(n) = O(n)

# Lower Bound
- la linea 4 è eseguita almeno n-3 volte

T(n) >= c4(n-3) = Theta(n)
=> T(n) = Omega(n)
=> T(n) = O(n) e T(n) = Omega(n) => T(n) = Theta(n)

Perchè è una grande idea la notazione asintotica:
- misura indipendete dall'implementazione dell'algoritmo dalla macchina reale
- i dettagli nascosti sono poco rilevanti per n molto grandi
- analisi dettagliata del numero di passi realmente eseguiti sarebbe difficile, noiosa e non direbbe molto di più
- descrive bene in pratica la velocità degli algoritmi

	


 





