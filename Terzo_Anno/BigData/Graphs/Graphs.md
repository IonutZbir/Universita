# Mining Graphs

## 1. Clustering

!!! note
    #### DEF (Clustering)
    - **INFORMALE** Il clustering è il processo di esaminare una collezione di "punti" e raggrupparli in "cluster" secondo una determinata misura di distanza. L'obiettivo è che i punti all'interno dello stesso cluster abbiano una distanza ridotta tra loro, mentre i punti appartenenti a cluster diversi presentino una distanza maggiore tra loro.

    - **FORMALE** Il clustering consiste nel prendere un insieme $\mathbb{U}$ di $n$ oggetti $p_1, p_2, \dots, p_n$ (che possono rappresentare foto, punti, documenti etc...) e classificarli in gruppi (cluster) coerenti. L'idea è dunque quella di dividere tali in punti in gruppi $C_1, C_2, \dots, C_k$ detti **cluster**, dove ciascun cluster rappresenta un sottoinsieme di oggetti simili.
      - $\mathbb{C} = \{C_1, C_2, \dots, C_k\}$
      - $C_i \subseteq U \forall i = 1, \dots, k$
      - $C_i \cap C_j = \emptyset\ \forall i,j = 1, \dots k$ tale che $i \neq j$
      - $\bigcup_{i = 1}^k C_i = U$
    
    #### DEF ($k$ - Clustering)
    Si dice $k$-clustering una partizione dell'insieme $\mathbb{U}$ $k$ sottoinsiemi non vuoti.

!!! note
    #### DEF (Distance Function)
    Per valutare la *vicinanza* tra due oggetti $p_i$ e $p_j$ viene definita una funzione di distanza $(d(p_i, p_j))$ che associa ad ogni coppia di oggetti $p_i$ e $p_j$ un valore numerico.

    #### Proprietà
    1. **Identità degli indiscernibili:** $d(p_i, p_j) = 0$ se e solo se $p_i = p_j$​
    2. **Non negatività:** $d(p_i, p_j) \geq 0$
    3. **Simmetria:** $d(p_i, p_j) = d(p_j, p_i)$

!!! note
    #### DEF (Spacing)
    Lo spacing rappresenta la **distanza minima tra due punti appartenenti a cluster differenti**.
    $$d(C_i, C_j) = \text{spacing}(C_i, C_j) = \min⁡\{d(p_i ,p_j):\ p_i \in C_i,\ p_j \in C_j\}$$
    Questo valore misura quanto sono *separati* due cluster.

    #### OBIETTIVO
    Dato un intero $k$, l'obiettivo è trovare un $k$-clustering che massimizza lo spacing tra i cluster. In pratica, si cerca di garantire che i cluster siano il più possibile distanti tra loro.

Il problema del clustering si può modellare mediante l'utilizzo di grafi pesati $G(V, E)$.
Dati in input $P = \langle p_1, p_2, \dots, p_n \rangle$, la funzione di distanza $d()$ e $k$, dobbiamo defnire il nostro modello di grafo su cui poi andare a creare i cluster.

- $V = \mathbb{U} = \{p_1​, \dots, p_n​\}$ è l'insieme dei nodi (oggetti da clusterizzare).
- $E$ è l'insieme di tutte le coppie non ordinate di nodi, formando un grafo completo.
- Ogni arco $e = (p_i, p_j)$ ha un costo $cost(e) = d(p_i, p_j)$, dove $d(p_i, p_j)$ è la funzione di distanza tra i punti.

L'algoritmo che risolve questo problema è il **Single-Link k-Clustering Algorithm**, un algoritmo greedy che costruisce la soluzione passo dopo passo seguendo una strategia semplice ma efficace. L'obiettivo è ottenere $k$ gruppi (o cluster) di punti mantenendo una separazione massima tra i gruppi.

L'idea di base è quella di immaginare i punti come nodi di un grafo. Inizialmente, ogni punto è considerato un cluster separato. Quindi, abbiamo un grafo con $n$ nodi e nessun arco. A questo punto, l'algoritmo procede in più passaggi:

1. A ogni iterazione, si cerca la coppia di punti appartenenti a cluster diversi che sono i più vicini tra loro. Questo significa che viene scelto l'arco con il costo più basso, dove il costo è la distanza tra i due punti. Questo arco viene aggiunto al grafo e i due cluster vengono fusi in uno solo.
2. Si continua a unire cluster seguendo lo stesso principio, ovvero scegliendo sempre la coppia di cluster più vicina. Questo processo si ripete $n − k$ volte, dove $k$ è il numero desiderato di cluster. Quando rimangono esattamente $k$ cluster, l'algoritmo si ferma.
3. A questo punto, i cluster rimanenti sono il risultato finale. Essi rappresentano gruppi ben distinti, con la distanza minima (spacing) tra punti di cluster diversi che è stata massimizzata durante il processo.

Questo metodo è praticamente identico all'algoritmo di **Kruskal**, usato per trovare il **Minimum Spanning Tree (MST)** di un grafo. Tuttavia, c'è una differenza chiave: mentre Kruskal continua fino a costruire l'intero MST, il single-link clustering si ferma non appena rimangono $k$ componenti connesse. Questo significa che, in pratica, stiamo costruendo un MST e poi rimuoviamo gli $k − 1$ archi più costosi per dividere il grafo in $k$ cluster.
